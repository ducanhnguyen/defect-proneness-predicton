{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names are file, loc, noc, bf, bug, document\n",
      "Processed 998 lines.\n",
      "internal core search indexing index binary folder \n"
     ]
    }
   ],
   "source": [
    "# Generate topics for class in JDT\n",
    "\n",
    "# configuration - begin\n",
    "NUM_TOPIC = 5\n",
    "inputFile = \"C:\\\\Users\\\\Duc Anh Nguyen\\\\Desktop\\\\jdt_document.csv\"\n",
    "outputFile = 'C:\\\\Users\\\\Duc Anh Nguyen\\\\Desktop\\\\jdt_topic.csv'\n",
    "# configuration - end\n",
    "\n",
    "import csv\n",
    "\n",
    "filename = []\n",
    "loc = []\n",
    "noc = []\n",
    "bf = []\n",
    "bug = []\n",
    "documents = []\n",
    "\n",
    "with open(inputFile) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            print(f'Column names are {\", \".join(row)}')\n",
    "            line_count += 1\n",
    "        else:\n",
    "            filename.append(row[0])\n",
    "            loc.append(row[1])\n",
    "            noc.append(row[2])\n",
    "            bf.append(row[3])\n",
    "            bug.append(row[4])\n",
    "            documents.append(row[5])\n",
    "            line_count += 1\n",
    "    print(f'Processed {line_count} lines.')\n",
    "    \n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Duc Anh\n",
      "[nltk_data]     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Duc Anh\n",
      "[nltk_data]     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['internal', 'core', 'search', 'indexing', 'index', 'binary', 'folder']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in documents] \n",
    "doc_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.184*\"internal\" + 0.133*\"compiler\" + 0.055*\"core\" + 0.035*\"util\" + 0.031*\"lookup\" + 0.026*\"info\" + 0.024*\"ast\" + 0.023*\"binding\" + 0.021*\"builder\" + 0.018*\"source\"'),\n",
       " (1,\n",
       "  '0.140*\"internal\" + 0.079*\"codeassist\" + 0.060*\"rewrite\" + 0.058*\"formatter\" + 0.058*\"comment\" + 0.050*\"selection\" + 0.046*\"select\" + 0.034*\"import\" + 0.028*\"node\" + 0.025*\"impl\"'),\n",
       " (2,\n",
       "  '0.214*\"core\" + 0.138*\"dom\" + 0.065*\"internal\" + 0.034*\"operation\" + 0.033*\"expression\" + 0.023*\"declaration\" + 0.022*\"statement\" + 0.022*\"package\" + 0.021*\"compilation\" + 0.019*\"member\"'),\n",
       " (3,\n",
       "  '0.151*\"internal\" + 0.122*\"core\" + 0.067*\"search\" + 0.053*\"type\" + 0.046*\"compiler\" + 0.042*\"ast\" + 0.035*\"reference\" + 0.031*\"matching\" + 0.020*\"java\" + 0.016*\"declaration\"'),\n",
       " (4,\n",
       "  '0.149*\"internal\" + 0.073*\"completion\" + 0.069*\"codeassist\" + 0.061*\"core\" + 0.059*\"complete\" + 0.039*\"code\" + 0.036*\"eval\" + 0.027*\"index\" + 0.024*\"snippet\" + 0.023*\"indexing\"')]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=NUM_TOPIC, id2word = dictionary, passes=50)\n",
    "\n",
    "ldamodel.print_topics(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5397598), (1, 0.028693667), (2, 0.028628027), (3, 0.20931694), (4, 0.1936016)]\n",
      "0.5397598\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# This is example\n",
    "# Get the best topic of a document\n",
    "document_id = 1\n",
    "x = ldamodel[doc_term_matrix[document_id]]\n",
    "highestProbTopic = 0\n",
    "highestTopic = 0\n",
    "print(x)\n",
    "for i in x:\n",
    "    if highestProbTopic < i[1] :\n",
    "        highestProbTopic = i[1]\n",
    "        highestTopic = i[0]\n",
    "\n",
    "print(highestProbTopic)\n",
    "print(highestTopic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Export to csv file\n",
    "with open(outputFile, mode='w', newline='') as FILE:\n",
    "    FILE_writer = csv.writer(FILE, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    # Create headers\n",
    "    headers = [\"file\", \"loc\", \"noc\", \"bf\", \"bug\", \"document\", \"topic\"]\n",
    "    for i in range(0, NUM_TOPIC):\n",
    "            headers.append(\"topic\" + str(i))\n",
    "            \n",
    "    FILE_writer.writerow(headers)\n",
    "    #\n",
    "    for i in range(0, len(filename)) :\n",
    "        # Find the best topic\n",
    "        predict = ldamodel[doc_term_matrix[i]]\n",
    "        highestProbTopic = 0\n",
    "        bestTopic = 0\n",
    "        for topic in predict:\n",
    "            if highestProbTopic < topic[1] :\n",
    "                highestProbTopic = topic[1]\n",
    "                bestTopic = topic[0]\n",
    "        \n",
    "        # write\n",
    "        output = [filename[i], loc[i], noc[i], bf[i], bug[i], documents[i], bestTopic]\n",
    "        for i in range(0, bestTopic):\n",
    "            output.append(0)\n",
    "        \n",
    "        output.append(1)\n",
    "        \n",
    "        for i in range(bestTopic+1, NUM_TOPIC):\n",
    "            output.append(0)\n",
    "            \n",
    "        FILE_writer.writerow(output)\n",
    "    \n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
